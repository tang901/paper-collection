# üìö Research Papers Collection

A comprehensive collection of papers for literature review and research purposes.

## üóÇÔ∏è Categories

- [Surrogate-Based Methods](#surrogate-based-methods)
- [Surrogate-Free Methods](#surrogate-free-methods)  
- [Benchmark & Empirical Studies](#benchmark--empirical-studies)

---

## ü§ñ Surrogate-Based Methods

*Methods that utilize external models or proxies for data selection.*

### External Model Based Approaches

| Paper Title | Authors | Venue | Year | Code |
|-------------|---------|-------|------|------|
| A Bayesian Approach to Data Point Selection | - | - | - | - |
| Active Learning for Effectively Fine-Tuning Transfer Learning to Downstream Task | - | - | - | - |
| Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete Space | - | - | - | - |
| Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering | - | - | - | - |
| AlpaGasus: Training A Better Alpaca with Fewer Data | - | - | - | - |
| Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation | - | - | - | - |
| CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs | - | - | - | - |
| CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training | - | - | - | - |
| Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach | - | - | - | - |

### Training Dynamics Based Methods

| Paper Title | Authors | Venue | Year | Code |
|-------------|---------|-------|------|------|
| LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning | - | - | - | - |
| LESS: Selecting Influential Data for Targeted Instruction Tuning | - | - | - | - |
| MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models | - | - | - | - |
| QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning | - | - | - | - |
| ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific Instruction Tuning | - | - | - | - |
| SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection | - | - | - | - |

---

## ‚ö° Surrogate-Free Methods

*Methods that do not rely on external models for data selection.*

### Uncertainty-Based Methods

| Paper Title | Authors | Venue | Year | Code |
|-------------|---------|-------|------|------|
| Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection | - | - | - | - |
| Fine-Tuning Language Models via Epistemic Neural Networks | - | - | - | - |
| GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to Filter Language Model Pretraining Data | - | - | - | - |
| STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models | - | - | - | - |
| UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection | - | - | - | - |
| Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning | - | - | - | - |
| Reasoning under Uncertainty: Efficient LLM Inference via Unsupervised Confidence Dilution and Convergent Adaptive Sampling | - | - | - | - |

### Gradient-Based Methods

| Paper Title | Authors | Venue | Year | Code |
|-------------|---------|-------|------|------|
| Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning Data Selection | - | - | - | - |
| CLUES: Collaborative High-Quality Data Selection for LLMs via Training Dynamics | - | - | - | - |
| ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs | - | - | - | - |
| Efficient Data Selection at Scale via Influence Distillation | - | - | - | - |
| G-DIG: Towards Gradient-based Diverse and High-quality Instruction Data Selection for Machine Translation | - | - | - | - |
| Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs | - | - | - | - |
| GORACS: Group-level Optimal Transport-guided Coreset Selection for LLM-based Recommender Systems | - | - | - | - |
| HyperINF: Unleashing the HyperPower of the Schulz's Method for Data Influence Estimation | - | - | - | - |
| Influential Language Data Selection via Gradient Trajectory Pursuit | - | - | - | - |
| ProDS: Preference-oriented Data Selection for Instruction Tuning | - | - | - | - |
| TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data | - | - | - | - |
| Towards Sustainable Learning: Coresets for Data-efficient Deep Learning | - | - | - | - |

### Diversity & Quality Based Methods

| Paper Title | Authors | Venue | Year | Code |
|-------------|---------|-------|------|------|
| 3DS: Decomposed Difficulty Data Selection's Case Study on LLM Medical Domain Adaptation | - | - | - | - |
| Boosting LLM via Learning from Data Iteratively and Selectively | - | - | - | - |
| Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning | - | - | - | - |
| DELIFT: DATA EFFICIENT LANGUAGE MODEL INSTRUCTION FINE-TUNING | - | - | - | - |
| D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning | - | - | - | - |
| From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning | - | - | - | - |
| Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs | - | - | - | - |
| INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models | - | - | - | - |
| P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for data pruning in LLM Training | - | - | - | - |
| ResoFilter: Fine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis | - | - | - | - |
| RICO: Refined In-Context Contribution for Automatic Instruction-Tuning Data Selection | - | - | - | - |
| Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning | - | - | - | - |
| The Best Instruction-Tuning Data are Those That Fit | - | - | - | - |
| Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality | - | - | - | - |

---

## üìä Benchmark & Empirical Studies

*Comparative studies and empirical analysis of data selection methods.*

| Paper Title | Authors | Venue | Year | Code |
|-------------|---------|-------|------|------|
| A Pre-trained Data Deduplication Model based on Active Learning | - | - | - | - |
| Active Code Learning: Benchmarking Sample-Efficient Training of Code Models | - | - | - | - |
| Active learning for reducing labeling effort in text classification tasks | - | - | - | - |
| Compute-Constrained Data Selection | - | - | - | - |
| Rethinking Data Selection at Scale: Random Selection is Almost All You Need | - | - | - | - |
| Rethinking Data Selection for Supervised Fine-Tuning | - | - | - | - |
| Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models | - | - | - | - |
| What is in Your Safe Data? Identifying Benign Data that Breaks Safety | - | - | - | - |
| What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning | - | - | - | - |
| Large-Scale Data Selection for Instruction Tuning | - | - | - | - |
| Can Data Diversity Enhance Learning Generalization? | - | - | - | - |

---

## üìù Statistics

- **Total Papers**: 100+
- **Surrogate-Based Methods**: 44 papers
- **Surrogate-Free Methods**: 32 papers  
- **Benchmark Studies**: 10 papers
- **Last Updated**: November 2024

## üîß Usage

This repository serves as a reference for:
- Literature review on data selection methods
- Research on active learning and data efficiency
- Comparative analysis of different approaches

## ü§ù Contributing

Feel free to contribute by:
1. Adding missing papers
2. Updating paper information (authors, venues, links)
3. Correcting any errors

---

<div align="center">
  
**Maintained for academic research purposes**

</div>
