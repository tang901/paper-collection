# üìö Research Papers Collection

A comprehensive collection of papers for literature review and research purposes.

## üóÇÔ∏è Categories

- [Surrogate-Based Methods](#surrogate-based-methods)
- [Surrogate-Free Methods](#surrogate-free-methods)  
- [Benchmark & Empirical Studies](#benchmark--empirical-studies)

---

## ü§ñ Surrogate-Based Methods

*Methods that utilize external models or proxies for data selection.*

- A Bayesian Approach to Data Point Selection (NeurIPS 2024)
- Active Learning for Effectively Fine-Tuning Transfer Learning to Downstream Task (Expert Systems with Applications 2025)
- Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete Space
- Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering
- AlpaGasus: Training A Better Alpaca with Fewer Data
- Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation
- CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs
- CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training
- Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach
- Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection
- CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom
- D3: Diversity, Difficulty, and Dependability-Aware Data Selection for Sample-Efficient LLM Instruction Tuning
- Data Diversity Matters for Robust Instruction Tuning
- Data Selection via Optimal Control for Language Models
- Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning
- DavIR: Data Selection via Implicit Reward for Large Language Models
- DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing
- Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement
- Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data
- Diversity Measurement and Subset Selection for Instruction Tuning Datasets
- Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder
- EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness
- Exploring the Mystery of Influential Data for Mathematical Reasoning
- FIRE: Flexible Integration of Data Quality Ratings for Effective Pre-Training
- GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to Filter Language Model Pretraining Data
- Harnessing Diversity for Important Data Selection in Pretraining Large Language Models
- How to Train Data-Efficient LLMs
- Improving Data Efficiency via Curating LLM-Driven Rating Systems
- InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities
- Instruction Mining: Instruction Data Selection for Tuning Large Language Models
- IterSelectTune: An Iterative Training Framework for Efficient Instruction-Tuning Data Selection
- LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning
- Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively Tuning Pre-trained Code Models
- LESS: Selecting Influential Data for Targeted Instruction Tuning
- MASS: MAthematical Data Selection via Skill Graphs for Pretraining Large Language Models
- MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models
- Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models
- MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space
- MINIPLM: KNOWLEDGE DISTILLATION FOR PRE-TRAINING LANGUAGE MODELS
- MODS: Model-oriented Data Selection for Instruction Tuning
- Task-Specific Data Selection for Instruction Tuning via Monosemantic Neuronal Activations
- Not All Documents Are What You Need for Extracting Instruction Tuning Data
- PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery
- Pay More Attention to the Robustness of Prompt for Instruction Data Mining
- Pre-trained Language Model Based Active Learning for Sentence Matching
- Predictive Data Selection: The Data That Predicts Is the Data That Teaches
- ProDS: Preference-oriented Data Selection for Instruction Tuning
- QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning
- QuRating: Selecting High-Quality Data for Training Language Models
- RAISE: Reinforced Adaptive Instruction Selection For Large Language Models
- Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning
- ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific Instruction Tuning
- Rule-based Data Selection for Large Language Models
- SCAR: Data Selection via Style Consistency-Aware Response Ranking for Efficient Instruction-Tuning of Large Language Models
- SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection
- SELECT2REASON: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning
- SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning
- SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models
- SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models
- Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy
- Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning
- Synth-Empathy: Towards High-Quality Synthetic Empathy Data
- T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning
- TACOS: Open Tagging and Comparative Scoring for Instruction Fine-Tuning Data Selection
- TAROT: Targeted Data Selection via Optimal Transport
- Task-Specific Data Selection for Instruction Tuning via Monosemantic Neuronal Activations
- The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph
- Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning
- ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection
- TSDS: Data Selection for Task-Specific Model Finetuning
- Speculative Coreset Selection for Task-Specific Fine-tuning
- Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data
- Data-efficient Fine-tuning for LLM-based Recommendation
- #InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models
- D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning
- Can Data Diversity Enhance Learning Generalization?

---

## ‚ö° Surrogate-Free Methods

*Methods that do not rely on external models for data selection.*

- Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection
- Improving Influence-based Instruction Tuning Data Selection for Balanced Learning of Diverse Capabilities
- 3DS: Decomposed Difficulty Data Selection's Case Study on LLM Medical Domain Adaptation
- Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning Data Selection
- Boosting LLM via Learning from Data Iteratively and Selectively
- CLUES: Collaborative High-Quality Data Selection for LLMs via Training Dynamics
- ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs
- Fine-Tuning Language Models via Epistemic Neural Networks
- STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models
- UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection
- Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning
- Reasoning under Uncertainty: Efficient LLM Inference via Unsupervised Confidence Dilution and Convergent Adaptive Sampling
- Efficient Data Selection at Scale via Influence Distillation
- G-DIG: Towards Gradient-based Diverse and High-quality Instruction Data Selection for Machine Translation
- Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs
- GORACS: Group-level Optimal Transport-guided Coreset Selection for LLM-based Recommender Systems
- HyperINF: Unleashing the HyperPower of the Schulz's Method for Data Influence Estimation
- Influential Language Data Selection via Gradient Trajectory Pursuit
- ProDS: Preference-oriented Data Selection for Instruction Tuning
- TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data
- Towards Sustainable Learning: Coresets for Data-efficient Deep Learning
- Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning
- DELIFT: DATA EFFICIENT LANGUAGE MODEL INSTRUCTION FINE-TUNING
- From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning
- Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs
- INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models
- P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for data pruning in LLM Training
- ResoFilter: Fine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis
- RICO: Refined In-Context Contribution for Automatic Instruction-Tuning Data Selection
- Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning
- The Best Instruction-Tuning Data are Those That Fit
- Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality


---

## üìä Benchmark & Empirical Studies

*Comparative studies and empirical analysis of data selection methods.*

- A Pre-trained Data Deduplication Model based on Active Learning
- Active Code Learning: Benchmarking Sample-Efficient Training of Code Models
- Active learning for reducing labeling effort in text classification tasks
- Compute-Constrained Data Selection
- Rethinking Data Selection at Scale: Random Selection is Almost All You Need
- Rethinking Data Selection for Supervised Fine-Tuning
- Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models
- What is in Your Safe Data? Identifying Benign Data that Breaks Safety
- What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning
- Large-Scale Data Selection for Instruction Tuning


---

## üìù Statistics

- **Total Papers**: 118
- **Surrogate-Based Methods**: 76 papers
- **Surrogate-Free Methods**: 32 papers  
- **Benchmark Studies**: 10 papers

## üîß Usage

This repository serves as a reference for literature review on data selection methods in machine learning.

## ü§ù Contributing

Feel free to contribute by adding missing papers or correcting any errors.

---

<div align="center">
  
**Maintained for academic research purposes**

</div>
