# üìö Research Papers Collection

A comprehensive collection of papers for literature review and research purposes.

## üóÇÔ∏è Categories

- [Surrogate-Based Methods](#surrogate-based-methods)
- [Surrogate-Free Methods](#surrogate-free-methods)  
- [Benchmark & Empirical Studies](#benchmark--empirical-studies)

---

## ü§ñ Surrogate-Based Methods

*Methods that utilize external models or proxies for data selection.*

- A Bayesian Approach to Data Point Selection (NeurIPS 2024)
- Active Learning for Effectively Fine-Tuning Transfer Learning to Downstream Task (Expert Systems with Applications 2025)
- Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete Space (EMNLP 2020)
- Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering (CoRR 2025)
- AlpaGasus: Training A Better Alpaca with Fewer Data (arXiv 2023)
- Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation (EMNLP 2024)
- CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs (arXiv 2024)
- CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training (NeurIPS 2024)
- Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach (ACL 2023)
- Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection (ICLR 2025)
- CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom (arXiv 2025)
- D3: Diversity, Difficulty, and Dependability-Aware Data Selection for Sample-Efficient LLM Instruction Tuning (IJCAI 2025)
- Data Diversity Matters for Robust Instruction Tuning (EMNLP 2024)
- Data Selection via Optimal Control for Language Models (ICLR 2025)
- Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning (arXiv 2025)
- DavIR: Data Selection via Implicit Reward for Large Language Models (ACL 2025)
- DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing (EMNLP 2024)
- Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement (arXiv 2024)
- Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data (arXiv 2025)
- Diversity Measurement and Subset Selection for Instruction Tuning Datasets (arXiv 2024)
- Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder (arXiv 2025)
- EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness (arXiv 2025)
- Exploring the Mystery of Influential Data for Mathematical Reasoning (COLM 2024)
- FIRE: Flexible Integration of Data Quality Ratings for Effective Pre-Training (arXiv 2025)
- GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to Filter Language Model Pretraining Data (arXiv 2024)
- Harnessing Diversity for Important Data Selection in Pretraining Large Language Models (arXiv 2024)
- How to Train Data-Efficient LLMs (CoRR 2024)
- Improving Data Efficiency via Curating LLM-Driven Rating Systems (ICLR 2025)
- InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities (arXiv 2025)
- Instruction Mining: Instruction Data Selection for Tuning Large Language Models (COLM 2024)
- IterSelectTune: An Iterative Training Framework for Efficient Instruction-Tuning Data Selection (arXiv 2024)
- LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning (arXiv 2025)
- Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively Tuning Pre-trained Code Models (ICSE 2024)
- LESS: Selecting Influential Data for Targeted Instruction Tuning (ICML 2024)
- MASS: MAthematical Data Selection via Skill Graphs for Pretraining Large Language Models (ICML 2025)
- MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models (NeurIPS 2024)
- Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models (ACL 2025)
- MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space (arXiv 2025)
- MINIPLM: KNOWLEDGE DISTILLATION FOR PRE-TRAINING LANGUAGE MODELS (ICLR 2025)
- MODS: Model-oriented Data Selection for Instruction Tuning (arXiv 2023)
- Task-Specific Data Selection for Instruction Tuning via Monosemantic Neuronal Activations (arXiv 2025)
- Not All Documents Are What You Need for Extracting Instruction Tuning Data (arXiv 2025)
- PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery (ICML 2025)
- Pay More Attention to the Robustness of Prompt for Instruction Data Mining (arXiv 2025)
- Pre-trained Language Model Based Active Learning for Sentence Matching (COLING 2020)
- Predictive Data Selection: The Data That Predicts Is the Data That Teaches (ICML 2025)
- ProDS: Preference-oriented Data Selection for Instruction Tuning (arXiv 2025)
- QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning (arXiv 2025)
- QuRating: Selecting High-Quality Data for Training Language Models (ICML 2024)
- RAISE: Reinforced Adaptive Instruction Selection For Large Language Models (arXiv 2025)
- Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning (arXiv 2025)
- ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific Instruction Tuning (EMNLP 2025)
- Rule-based Data Selection for Large Language Models (arXiv 2024)
- SCAR: Data Selection via Style Consistency-Aware Response Ranking for Efficient Instruction-Tuning of Large Language Models (ACL 2025)
- SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection (ICLR 2025)
- SELECT2REASON: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning (arXiv 2025)
- SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning (NeurIPS 2024)
- SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models (arXiv 2025)
- SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models (NeurIPS 2024)
- Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy (EMNLP 2025)
- Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning (ACL 2024)
- Synth-Empathy: Towards High-Quality Synthetic Empathy Data (arXiv 2024)
- T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning (arXiv 2025)
- TACOS: Open Tagging and Comparative Scoring for Instruction Fine-Tuning Data Selection (arXiv 2025)
- TAROT: Targeted Data Selection via Optimal Transport (arXiv 2024)
- Task-Specific Data Selection for Instruction Tuning via Monosemantic Neuronal Activations (arXiv 2025)
- The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph (ICML 2025)
- Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning (arXiv 2025)
- ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection (arXiv 2025)
- TSDS: Data Selection for Task-Specific Model Finetuning (NeurIPS 2024)
- Speculative Coreset Selection for Task-Specific Fine-tuning (CoRR 2024)
- Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data (neurips2023)
- Data-efficient Fine-tuning for LLM-based Recommendation (SIGIR 2024)
- #InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models (NeurIPS 2023)
- D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning (arxiv 2023)
- Can Data Diversity Enhance Learning Generalization? (ACL 2022)

---

## ‚ö° Surrogate-Free Methods

*Methods that do not rely on external models for data selection.*

- Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection (ENLSP-III workshop at NeurIPS 2023)
- Improving Influence-based Instruction Tuning Data Selection for Balanced Learning of Diverse Capabilities (ICLR2025)
- 3DS: Decomposed Difficulty Data Selection's Case Study on LLM Medical Domain Adaptation (arXiv 2024)
- Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning Data Selection (arXiv 2025)
- Boosting LLM via Learning from Data Iteratively and Selectively (arXiv 2024)
- CLUES: Collaborative High-Quality Data Selection for LLMs via Training Dynamics (arXiv 2025)
- ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs (arXiv 2025)
- Fine-Tuning Language Models via Epistemic Neural Networks (arXiv 2022)
- STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models (ACL 2024)
- UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection (arXiv 2025)
- Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning (arXiv 2025)
- Reasoning under Uncertainty: Efficient LLM Inference via Unsupervised Confidence Dilution and Convergent Adaptive Sampling 
- Efficient Data Selection at Scale via Influence Distillation (arXiv 2025)
- G-DIG: Towards Gradient-based Diverse and High-quality Instruction Data Selection for Machine Translation (ACL 2024)
- Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs (ICLR 2024)
- GORACS: Group-level Optimal Transport-guided Coreset Selection for LLM-based Recommender Systems (KDD 2025)
- HyperINF: Unleashing the HyperPower of the Schulz's Method for Data Influence Estimation (CoRR 2024)
- Influential Language Data Selection via Gradient Trajectory Pursuit (ICLR 2025)
- ProDS: Preference-oriented Data Selection for Instruction Tuning (arXiv 2025)
- TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data (NAACL 2025)
- Towards Sustainable Learning: Coresets for Data-efficient Deep Learning (ICML 2023)
- Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning (arXiv 2025)
- DELIFT: DATA EFFICIENT LANGUAGE MODEL INSTRUCTION FINE-TUNING (ICLR 2025)
- From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning (NAACL 2024)
- Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs (arXiv 2025)
- INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models (EMNLP 2023)
- P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for data pruning in LLM Training (arXiv 2024)
- ResoFilter: Fine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis (NAACL 2025)
- RICO: Refined In-Context Contribution for Automatic Instruction-Tuning Data Selection (arXiv 2025)
- Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning (ACL 2024)
- The Best Instruction-Tuning Data are Those That Fit (arXiv 2025)
- Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality (ICLR 2024)

---

## üìä Benchmark & Empirical Studies

*Comparative studies and empirical analysis of data selection methods.*

- A Pre-trained Data Deduplication Model based on Active Learning (Expert Systems with Applications 2025)
- Active Code Learning: Benchmarking Sample-Efficient Training of Code Models (IEEE Transactions on Software Engineering 2024)
- Active learning for reducing labeling effort in text classification tasks (BNAIC/Benelearn 2021)
- Compute-Constrained Data Selection (ICLR 2025)
- Rethinking Data Selection at Scale: Random Selection is Almost All You Need (ICLR 2025)
- Rethinking Data Selection for Supervised Fine-Tuning (arXiv 2024)
- Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models (NAACL 2025)
- What is in Your Safe Data? Identifying Benign Data that Breaks Safety (COLM 2024)
- What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning (ICLR 2024)
- Large-Scale Data Selection for Instruction Tuning (CoRR 2025)

---

## üìù Statistics

- **Total Papers**: 118
- **Surrogate-Based Methods**: 76 papers
- **Surrogate-Free Methods**: 32 papers  
- **Benchmark Studies**: 10 papers

## üîß Usage

This repository serves as a reference for literature review on data selection methods in machine learning.

## ü§ù Contributing

Feel free to contribute by adding missing papers or correcting any errors.

---

<div align="center">
  
**Maintained for academic research purposes**

</div>
